# 학습 회고

## Mon - Day 22
- **Peer-Session**
    - Transformer - postition encoding
    - Naive Bayes classifier 실습 코드에서 로그 확률(Log probability)를 사용하는 이유는 단어의 수가 많아질수록, 확률이 0에 가깝게 수렴하기 때문에 로그 가능도를 사용
    - word2vec 
- **Fact**  
: NLP 이론 part 1 학습 (Word Embedding)
- **Feeling**  
: 본격적으로 NLP에 대해 배운 첫날 아직 완전히 이해한 것이 너무 적다. 앞부분을 제대로 이해하고 step by step을 하고 싶은데 용 힘들긴하다. 마스터 클래스에서 안수빈 마스터님의 이력을 듣고 내가 이 분야에서 먹고 살수 있을지 걱정이 되긴 하다. 영어도 해야되는데... 뭐 나중가면 다 잘 되겠지 하다가도 공부하다보면 다시 걱정이 된다. 피어세션에서 푸아그라를 만드는 거위처럼 지식을 받고 있다라는 말이 있었는데 동감이 된다. 그래도 뭐 나중가면 소화 다 되겠지. 기왕이면 때깔 좋은 거위가 되자
- **Future Plan & Finding**  
: github 특강도 있는 주라 시간이 빠듯하게 써서 피어세션 때 동료들에게 가르쳐 줄 정도로 습득하자

## Tue - Day 23
- **Peer-Session**  
    - GitHub 특강 관련 질의
    - 멘토링 과제 관련 질의
- **Fact**  
: GitHub 특강, Visualization 4강 학습
- **Feeling**  
: git을 쓰고 있었지만 개념적으로 불안한면이 있었는데 오늘 특강으로 git에 대한 개념이 확실히 잡힌 것 같다. 강사님이 복습하면 5분이면 된다고 하셨지만, 많이 쓰는 버전관리시스템인만큼 앞으로 나아갈 단단한 기본을 들고 가는 것 같다. 5시간짜리 강의였던만큼 힘은 들었지만 좋았다.
- **Future Plan & Finding**  
: NLP 이론 part2 수강, Data visualization 수강


## Wed - Day 24
- **Peer-Session**  
    - 초기 가중치 방법에 따른 학습 성능
    - Word2Vec은 단어의 분산 표현을 학습하고 단어 간 유사성을 파악하는 데 중점을 두는 반면, Transformer Attention은 문맥을 파악하고 문장 내 단어들 간의 상호작용을 이해하여 다양한 시퀀스 투 시퀀스 모델에서 활용됩니다.
- **Fact**  
: NLP 이론 part2 학습, 기본과제 1
- **Feeling**  
: 피곤하다. 어제 운동을 빡세게 했나보다. 교수님의 강의가 마치 자장가 같이 들려서 정신줄을 몇번이고 놓았다. 1시간 강의를 뒤로가기를 얼마나 눌렀는지 1시간 30분은 훌쩍 넘은 듯하다. 
- **Future Plan & Finding**  
: 기본과제를 풀었는데 정규식으로 해결하고 싶어서 시간을 꽤 썼다. 정규식 관련 강의도 좀 찾아서 봐야겠다. 피어세션 때 Transformer의 Attention 이 나왔는데 며칠 안봤다고 좀 낯설다. 이번주 목표치를 빨리 수강하고 복습해야겠다.


## Thu - Day 25
- **Peer-Session**  
    - Seq2Seq의 이론 및 역전파 이해
    - 기본과제 1에 관한 고찰
    - 오프라인 참가 여부
- **Fact**  
: NLP 이론 part3 학습, 기본과제 2
- **Feeling**  
: 기본과제를 맞게 했는지 모르겠다. 디버깅 열실히 하고 학습도 열심히 돌렸는데 성능이 안나온다. 이 새벽에 결과치를 문득 보니 전처리에 문제가 있는 것 같기도 하다. 이론적으론 교수님의 강의를 통해 이해가 되는데 코드는 따라가기 힘들다.ㅜ 
- **Future Plan & Finding**  
: 멘토링 과제해야한다~ visualization 강의 아직 다 안들었다~


## Fri - Day 26
- **Peer-Session**  
    -
- **Fact**  
: 
- **Feeling**  
: 
- **Future Plan & Finding**  
: 



