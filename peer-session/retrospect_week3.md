# 학습 회고

## Mon - Day 15
- **Peer-Session**  
    - 지난주의 배웠던 gather에 대해 서로 얘기하며 개념을 잡아갔다.  
    - zero_grad()를 호출해야하는 이유에 대해 PyTorch가 그래디언트를 누적하기 때문이다. 이러한 누적 동작이 RNN(순환 신경망)을 학습할 때나 여러 미니 배치에 걸쳐 손실의 그래디언트를 계산할 때 편리하기에 기본적으로 파이토치는 loss.backward()를 호출할 때마다 그래디언트를 누적하도록 설정되어있다.
- **Fact**  
: 1,2,3 강 강의를 학습하였다. 
- **Feeling**  
: 수업에서 다루지 않은 내용에 의문을 품고 동료들과 같이 개념을 하나씩 쌓아갔다. 피어세션은 점차 발전하는데 개인으로는 어떤지 잘 모르겠다. 월요병인지 오후가 되니 집중력이 많이 떨어졌다. 오늘 회고를 쓰며 반성해본다.
- **Future Plan & Finding**  
: 본격적인 torch 코드가 나와서 해당 코드에 익숙해지며 이전의 수학과 접목해 학습해야겠다.


## Tue - Day 16
- **Peer-Session**  
    - 오늘 배운 내용에 관련하여 서로서로 개념 체크
    - googlenet에서 1x1 conv 을 통해 채널 수를 줄이는 방식이 정보 소실이 되지는 않는다.
    - NLP와 관련된 논문을 읽어보는 것이 좋겠다.
- **Fact**  
: 4,5,6,7 강 강의를 학습하였다. CNN과 RNN에 관해 여러 이론들을 듣고 실습하였다.
- **Feeling**  
: 지금까지 나온 코드들을 하나하나 이해하기도 아직 버겁다. 언젠가 배운 개념과 코드들이 익숙해져서 아이디어를 내 손으로 팍팍 구현하길 바란다. 조금씩 조금씩 좀 더 나아지자. 확실히 어제보단 오늘이 괜찮게 공부한 것 같다 ㅎㅎ
- **Future Plan & Finding**  
: 오늘 배운 것들의 많은 부분들이 Transformer로 대체되고 있다고 한다. 다음 강의인 transformer에 대해 확실히 이해하고 넘어가야겠다. Data Visualization 강의는 코어타임 외에 시간을 할애하여 수강하겠다.


## Wed - Day 17
- **Fact**  
: 
- **Feeling**  
: 
- **Future Plan & Finding**  
: 


## Thu - Day 18
- **Fact**  
: 
- **Feeling**  
: 
- **Future Plan & Finding**  
: 


## Fri - Day 19
- **Fact**  
: 
- **Feeling**  
: 
- **Future Plan & Finding**  
: 



