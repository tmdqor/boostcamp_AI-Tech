# 확률론 (probability theory)

## 딥러닝에서 확률론이 필요한 이유
* 딥러닝은 확률론 기반의 기계학습 이론에 바탕을 두고있음
* 기계학습에서 사용되는 loss function의 작동원리는 데이터 공간을 통계적으로 해석해서 유도
* 회귀분석에서 손실함수로 사용되는 L2노름은 예측오차의 분산을 가장 최소화하는 방향으로 학습하도록 유도
* 분류 문제에서 사용되는 cross-entropy는 모델 예측의 불확실성을 최소화하는 방향으로 학습하도록 유도
* 분산 및 불확실성을 최소화하기 위해서는 측정하는 방법을 알아야함

## 확률분포
* 데이터공간을 X*Y라고 표기하고 D는 데이터공간에서 데이터를 추출하는 분포
* 데이터는 확률변수(x,y) ~ D라표기

## 확률변수 종류
* 확률변수의 분포의 종류(D)에 의해 구분
* 이산형확률변수 (discrete)
    * 확률 변수가 가질 수 있는 경우의 수를 모두 고려하여 확률을 더해서 모델링함
    * 확류변수X가 A라는 집합에 속할 확률분포 = 확류변수 X가 x가 될 모든 경우의 수를 고려하여 이를 모두 더해서 확률분포를 모델링
    * 확률 질량함수라고 부름
* 연속형확률변수 (continous)
    * 데이터 공간에 정의된 확률변수의 밀도위에서의 적분을 통해 확률분포를 모델링
    * X가 A라는 집합에 속할 확률을 계산할때 A상에서 P(X) 적분한다
    * P(X)는 누적확률분포의 변화율을 모델링

## 결합분포 P(x,y)
* 결합분포 P(x,y)는 D를 모델링한다
* 원래 확률분포 D가 이산형일지라도 연속형확률분포로 모델링 해볼 수 있고 연속형 일지라도 이산형으로 모델링 해볼 수 있음

## 주변확률분포 P(x)
* 입력 X에대한 주변확률분포는 X에대한 정보만 주고 Y에 대한 정보는 주지않음

## 조건부확률분포 P(x|y)
* y가 주어진 상태에서 x에 대한 확률분포를 구함
* 특정 클래스가 주어진 조건에서 데이터의 확률분포를 보여줌
* 조건부 확률 P(y|x)는 입력변수 x에대해 정답이 y일 확률을 말함
* 하지만 연속확률 분포 P(y|x)일 경우 확률이 아니라 밀도로 해석함
* P(y|x)는 x가 주어졌을 때 y를 구한다

## 조건부 기대값 E[y|x]
* 회귀문제의 경우 조건부 기대값 E[y|x]을 추정
* 이산확률변수인경우 y를 곱해준 후 더해주면 조건부 기대값을 계산 가능
* 회귀문제는 연속확률 변수인 경우를 다루기 때문에 이 경우에는 적분으로 계산
* 사용하는이유: 목적식으로 사용하게되는 L2 노름을 최소화하는 함수와 일치하기 때문

## 기대값(expectatio n)
* 확률분포가 주어지면 데이터를 분석하는데 사용가능한 여러종류의 통계적 범함수(statistical functional)를 계산하는데 이용되는 도구
* 기대값은 평균(mean)과 동일하게 사용되고 더 넓게도 사용됨
* 기대값은 데이터를 대표하는 통계량이면서 동시에 확률분포를 통해 다른 통계적 범함수를 계산하는데 사용
* 연속확률분포의 경우엔 적분 이산확률분포의 경우엔 급수(summation)를 사용하여 기대값을 정의

## 몬테카를로 샘플링
* 기계학습의 많은 문제들은 확률분포를 명시적으로 모를대가 대부분임
* 확률분포를 모를때 사용됨
* 데이터를 이용해서 기대값을 계산
* 샘플링만 가능하다면 기대값을 계산할 수 있음